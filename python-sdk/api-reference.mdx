---
title: 'API Reference'
description: 'Complete Python SDK API reference for Osmosis AI'
---

# API Reference

Complete reference for the osmosis-ai Python SDK.

## Decorators

### @osmosis_reward

Decorator for local reward functions that compute scores without API calls.

**Signature:**
```python
@osmosis_reward
def function_name(
    solution_str: str,
    ground_truth: str,
    extra_info: dict = None,
    **kwargs
) -> float
```

**Parameters:**
- `solution_str` (str, required) - Text to evaluate
- `ground_truth` (str, required) - Reference answer
- `extra_info` (dict, optional) - Additional context
- `**kwargs` (required) - Future compatibility (see warning below)

**Returns:** `float` - Score value

<Snippet file="_includes/kwargs-requirement.mdx" />

**Example:**
```python
from osmosis_ai import osmosis_reward

@osmosis_reward
def exact_match(solution_str: str, ground_truth: str, extra_info: dict = None, **kwargs) -> float:
    return 1.0 if solution_str.strip() == ground_truth.strip() else 0.0
```

---

### @osmosis_rubric

Decorator for LLM-based evaluation functions.

**Signature:**
```python
@osmosis_rubric
def function_name(
    solution_str: str,
    ground_truth: str | None,
    extra_info: dict,
    **kwargs
) -> float
```

**Parameters:**
- `solution_str` (str, required) - Text to evaluate
- `ground_truth` (str | None, required) - Reference answer (can be None)
- `extra_info` (dict, required) - Configuration and context
- `**kwargs` (required) - Future compatibility (see warning below)

**Returns:** `float` - Score value

<Snippet file="_includes/kwargs-requirement.mdx" />

**Example:**
```python
from osmosis_ai import osmosis_rubric, evaluate_rubric

@osmosis_rubric
def quality_check(solution_str: str, ground_truth: str | None, extra_info: dict, **kwargs) -> float:
    return evaluate_rubric(
        rubric="Evaluate response quality",
        solution_str=solution_str,
        model_info={"provider": "openai", "model": "gpt-5"},
        ground_truth=ground_truth
    )
```

---

## Core Functions

### evaluate_rubric()

Evaluate text using an LLM-based rubric.

**Signature:**
```python
def evaluate_rubric(
    rubric: str,
    solution_str: str,
    model_info: dict,
    ground_truth: str | None = None,
    original_input: str | None = None,
    metadata: dict | None = None,
    score_min: float = 0.0,
    score_max: float = 1.0,
    timeout: int | None = None,
    return_details: bool = False
) -> float | dict
```

**Parameters:**

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `rubric` | str | Yes | Natural language evaluation criteria |
| `solution_str` | str | Yes | Text to evaluate |
| `model_info` | dict | Yes | LLM provider configuration |
| `ground_truth` | str | No | Reference answer |
| `original_input` | str | No | Original user query |
| `metadata` | dict | No | Additional context |
| `score_min` | float | No | Minimum score (default: 0.0) |
| `score_max` | float | No | Maximum score (default: 1.0) |
| `timeout` | int | No | Request timeout in seconds |
| `return_details` | bool | No | Return full response (default: False) |

**model_info Structure:**
```python
{
    "provider": "openai",           # Required
    "model": "gpt-5",         # Required
    "api_key": "sk-...",            # Optional
    "api_key_env": "OPENAI_API_KEY", # Optional
    "timeout": 30                   # Optional
}
```

**Returns:**
- `float` - Score (when `return_details=False`)
- `dict` - Full response with score, explanation, raw payload (when `return_details=True`)

**Example:**
```python
from osmosis_ai import evaluate_rubric

score = evaluate_rubric(
    rubric="Evaluate how helpful the response is.",
    solution_str="Click 'Forgot Password' to reset.",
    model_info={"provider": "openai", "model": "gpt-5"}
)
```

---

## Exceptions

### MissingAPIKeyError

Raised when an API key is not found for a provider.

```python
from osmosis_ai import MissingAPIKeyError

try:
    score = evaluate_rubric(...)
except MissingAPIKeyError as e:
    print(f"API key not found: {e}")
```

---

### ProviderRequestError

Raised when a provider request fails.

```python
from osmosis_ai import ProviderRequestError

try:
    score = evaluate_rubric(...)
except ProviderRequestError as e:
    print(f"Provider error: {e}")
```

---

### ModelNotFoundError

Raised when a specified model is not available (subclass of `ProviderRequestError`).

```python
from osmosis_ai import ModelNotFoundError

try:
    score = evaluate_rubric(...)
except ModelNotFoundError as e:
    print(f"Model not found: {e}")
```

---

<Snippet file="_includes/supported-providers.mdx" />

---

## Type Definitions

### ModelInfo (TypedDict)

```python
from osmosis_ai.rubric_types import ModelInfo

model_info: ModelInfo = {
    "provider": "openai",
    "model": "gpt-4o",
    "api_key_env": "OPENAI_API_KEY",
    "timeout": 30
}
```

---

### RewardRubricRunResult (TypedDict)

Returned when `return_details=True`:

```python
from osmosis_ai.rubric_types import RewardRubricRunResult

result: RewardRubricRunResult = {
    "score": 0.85,              # float
    "explanation": "...",       # str
    "raw": {...}                # Any - raw LLM response
}
```

---

## Remote Rollout Utilities

### Tool Helpers

The SDK provides utility functions for working with tool calls in remote rollout agents:

```python
from osmosis_ai.rollout.tools import (
    create_tool_result,
    create_tool_error_result,
    get_tool_call_info,
    parse_tool_arguments,
    execute_tool_calls,
    serialize_tool_result,
)
```

| Function | Description |
|----------|-------------|
| `create_tool_result(tool_call_id, content)` | Create a tool result message |
| `create_tool_error_result(tool_call_id, error, is_error=True)` | Create an error tool result |
| `get_tool_call_info(tool_call)` | Extract `(id, name, arguments)` from tool call |
| `parse_tool_arguments(arguments)` | Parse tool arguments (handles string/dict) |
| `execute_tool_calls(tool_calls, executor)` | Execute multiple tool calls in parallel |
| `serialize_tool_result(result)` | Convert result to string for tool response |

**Example:**
```python
from osmosis_ai.rollout.tools import get_tool_call_info, create_tool_result

async def execute_tool(tool_call):
    tool_call_id, name, args = get_tool_call_info(tool_call)
    result = await my_tool_handler(name, args)
    return create_tool_result(tool_call_id, str(result))
```

---

### Message Helpers

Utilities for working with chat messages:

```python
from osmosis_ai.rollout.messages import (
    get_message_content,
    get_message_role,
    is_assistant_message,
    is_tool_message,
    is_user_message,
    count_messages_by_role,
    parse_tool_calls,
)
```

| Function | Description |
|----------|-------------|
| `get_message_content(message)` | Safely get message content (handles None) |
| `get_message_role(message)` | Get message role |
| `is_assistant_message(message)` | Check if message is from assistant |
| `is_tool_message(message)` | Check if message is a tool result |
| `is_user_message(message)` | Check if message is from user |
| `count_messages_by_role(messages)` | Count messages by role |
| `parse_tool_calls(messages)` | Extract tool calls from messages |

---

### Network Utilities

Functions for network operations in remote rollout servers:

```python
from osmosis_ai.rollout.network import (
    detect_public_ip,
    validate_ipv4,
    is_valid_hostname_or_ip,
    is_private_ip,
    PublicIPDetectionError,
)
```

| Function | Description |
|----------|-------------|
| `detect_public_ip()` | Detect the server's public IP address |
| `validate_ipv4(ip_string)` | Validate an IPv4 address string |
| `is_valid_hostname_or_ip(hostname)` | Check if string is valid hostname or IP |
| `is_private_ip(ip)` | Check if IP address is private (RFC 1918) |

**Example:**
```python
from osmosis_ai.rollout.network import detect_public_ip, PublicIPDetectionError

try:
    public_ip = detect_public_ip()
    print(f"Server public IP: {public_ip}")
except PublicIPDetectionError as e:
    print(f"Could not detect public IP: {e}")
```

---

### Validation

Validate agent loop implementations before deployment:

```python
from osmosis_ai.rollout import validate_agent_loop, ValidationResult

result: ValidationResult = validate_agent_loop(my_agent)
if result.is_valid:
    print(f"Agent '{result.agent_name}' is valid")
    print(f"Tools: {result.tool_count}")
else:
    for error in result.errors:
        print(f"Error: {error}")
```

---

## Complete Example

```python
from osmosis_ai import osmosis_reward, osmosis_rubric, evaluate_rubric
from dotenv import load_dotenv

load_dotenv()

# Local reward function
@osmosis_reward
def exact_match(solution_str: str, ground_truth: str, extra_info: dict = None, **kwargs) -> float:
    return 1.0 if solution_str.strip() == ground_truth.strip() else 0.0

# Remote rubric evaluator
@osmosis_rubric
def semantic_eval(solution_str: str, ground_truth: str | None, extra_info: dict, **kwargs) -> float:
    return evaluate_rubric(
        rubric="Compare semantic similarity (0-1 scale)",
        solution_str=solution_str,
        ground_truth=ground_truth,
        model_info={"provider": "openai", "model": "gpt-5"}
    )

# Usage
solution = "The capital of France is Paris"
truth = "Paris is France's capital"

local_score = exact_match(solution, truth)
semantic_score = semantic_eval(solution, truth, {})

print(f"Exact match: {local_score}")      # 0.0
print(f"Semantic: {semantic_score}")      # ~1.0
```

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Quick Start" icon="rocket" href="/python-sdk/quickstart">
    Learn with examples
  </Card>
  <Card title="Decorators & API Guide" icon="code" href="/python-sdk/decorators-api">
    Advanced patterns
  </Card>
  <Card title="CLI Reference" icon="terminal" href="/python-sdk/cli-reference">
    Batch evaluations
  </Card>
</CardGroup>
