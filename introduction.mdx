---
title: 'Introduction'
description: 'Osmosis helps companies create task-specific models that beat foundation models at a fraction of the cost.'
---

Welcome to Osmosis — the forward-deployed reinforcement learning platform that helps you create task-specific models beating foundation models at a fraction of the cost.

You define how your agent should behave — the tools it can use, how its outputs are scored, and what data it trains on — and Osmosis handles the GPU infrastructure, training loops, and model management. The result: models that get measurably better at your specific tasks over time, without manual fine-tuning.

## How It Works

<Steps>
  <Step title="You Define">
    Provide the building blocks for training:
    - **Tools & Agent Logic** — the actions your agent can take
    - **Reward Functions** — how outputs are scored
    - **Training Data** — the tasks your model learns from
  </Step>
  <Step title="Osmosis Trains">
    The platform handles the heavy lifting:
    - **GPU Training Cluster** — managed infrastructure, no setup needed
    - **RL Training Loop** — GRPO, DAPO, and multi-turn tool training
    - **Checkpoints & Metrics** — track progress in real time
  </Step>
  <Step title="Deploy Your Model">
    Ship a model that's better at your tasks:
    - **Merge to HuggingFace** — export trained weights
    - **Deploy Anywhere** — use your model in any environment
  </Step>
</Steps>

## Get Started

<CardGroup cols={2}>
  <Card title="Platform Guide" icon="grid-2" href="/platform/overview">
    Learn about the Osmosis Platform — workspaces, training runs, metrics, and model management.
  </Card>
  <Card title="Python SDK" icon="python" href="/python-sdk/introduction">
    Install the SDK to build reward functions, agent loops, and CLI tools for local testing.
  </Card>
  <Card title="Remote Rollout" icon="server" href="/remote-rollout/overview">
    Build custom agent servers that integrate with Osmosis training infrastructure.
  </Card>
  <Card title="Local Rollout" icon="code-branch" href="/git-sync/overview">
    Sync reward functions, rubrics, and MCP tools from your GitHub repository.
  </Card>
</CardGroup>

## What is a Rollout?

In reinforcement learning, a **rollout** is the process of running a policy in an environment to generate a **trajectory** — the complete sequence of actions, observations, and outcomes from start to finish. In the LLM context, a rollout is a single attempt by the model to solve a task, including any reasoning steps, tool usage, and final output. Think of it like a single ChatGPT conversation: if multiple users ask the same question to the same model, each interaction counts as a separate rollout.

Each rollout produces a trajectory that captures everything the model did during that attempt. A **reward function** then scores how well the model performed. Osmosis collects these trajectories and rewards, then uses reinforcement learning (GRPO, DAPO) to update the model's policy — nudging it toward strategies that earn higher rewards.

By running thousands of rollouts per training iteration, the model discovers which reasoning patterns, tool-use strategies, and response styles lead to better outcomes — and improves measurably on your specific tasks over time.

## Choose Your Workflow

Osmosis supports two main workflows for connecting your code to the training platform:

| | Local Rollout | Remote Rollout |
|---|---|---|
| **Best for** | Reward functions, rubrics, MCP tools | Custom agent loops with complex logic |
| **How it works** | Push to GitHub → auto-synced to platform | Run your own HTTP server → platform connects |
| **Setup** | Add decorators + folder structure | Implement `RolloutAgentLoop` |
| **When to use** | Standard tool-use training | Multi-step reasoning, custom environments |

## Quick Links

- [Sign up for Osmosis](https://platform.osmosis.ai)
- [Contact us](https://osmosis.ai/contact)
